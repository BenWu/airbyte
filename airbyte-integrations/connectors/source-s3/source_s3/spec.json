{
  "documentationUrl": "https://docs.airbyte.io/integrations/sources/s3",
  "connectionSpecification": {
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "S3 Source Spec",
    "type": "object",
    "additionalProperties": false,
    "required": ["dataset_name", "provider", "format", "path_patterns"],
    "properties": {
      "dataset_name": {
        "type": "string",
        "description": "This source creates one table per connection, this field is the name of that table. This should include only letters, numbers, dash and underscores. Note that this may be altered according to destination."
      },
      "path_patterns": {
        "type": "array",
        "description": "Add at least 1 pattern here to match filepaths against. Airbyte uses these patterns to determine which files to pick up from the provider storage. See <a href=\"https://docs.python.org/3/library/fnmatch.html\" target=\"_blank\">Python's fnmatch</a> to understand pattern syntax. Use pattern <strong>*</strong> to pick up all files.",
        "examples": ["*", "myFolder/myTableFiles/*.csv"]
      },
      "schema" : {
        "type": "string",
        "default": "{}",
        "description": "Optionally provide a schema to enforce, as a valid JSON string. Ensure this is a mapping of <strong>{ \"column\" : \"type\" }</strong>, where types are valid <a href=\"https://json-schema.org/understanding-json-schema/reference/type.html\" target=\"_blank\">JSON Schema datatypes</a>. Leave as {} to auto-infer the schema.",
        "examples": ["{\"column_1\": \"number\", \"column_2\": \"string\", \"column_3\": \"array\", \"column_4\": \"object\", \"column_5\": \"boolean\"}"]
      },
      "provider": {
        "type": "object",
        "description": "Storage Provider.",
        "oneOf": [
          {
            "title": "S3: Amazon Web Services",
            "required": ["storage", "bucket"],
            "properties": {
              "storage": {
                "type": "string",
                "enum": ["S3"],
                "default": "S3"
              },
              "bucket": {
                "type": "string",
                "description": "Name of the S3 bucket where the file(s) exist."
              },
              "aws_access_key_id": {
                "type": ["null", "string"],
                "default": null,
                "description": "In order to access private Buckets stored on AWS S3, this connector requires credentials with the proper permissions. If accessing publicly available data, this field is not necessary.",
                "airbyte_secret": true
              },
              "aws_secret_access_key": {
                "type": ["null", "string"],
                "default": null,
                "description": "In order to access private Buckets stored on AWS S3, this connector requires credentials with the proper permissions. If accessing publicly available data, this field is not necessary.",
                "airbyte_secret": true
              },
              "path_prefix": {
                "type": "string",
                "description": "By providing a path-like prefix (e.g. myFolder/thisTable/) under which all the relevant files sit, we can optimise finding these in S3. This is optional but recommended if your bucket contains many folders/files."
              }
            }
          }
        ]
      },
      "format": {
        "type": "object",
        "description": "File format and associated settings for the files you want to sync.",
        "oneOf": [
          {
            "title": "csv",
            "required": ["filetype"],
            "properties": {
              "filetype": {
                "type": "string",
                "enum": ["csv"],
                "default": "csv",
                "description": "This connector utilises <a href=\"https://arrow.apache.org/docs/python/generated/pyarrow.csv.open_csv.html\" target=\"_blank\">PyArrow (Apache Arrow)</a> for CSV parsing."
              },
              "delimiter": {
                "type": "string",
                "default": ",",
                "minLength": 1,
                "description": "The character delimiting individual cells in the CSV data. This may only be a 1-character string."
              },
              "quote_char": {
                "type": "string",
                "default": "\"",
                "description": "The character used optionally for quoting CSV values. To disallow quoting, make this field blank."
              },
              "escape_char": {
                "type": ["null", "string"],
                "default": null,
                "description": "The character used optionally for escaping special characters. To disallow escaping, leave this field blank."
              },
              "encoding": {
                "type": ["null", "string"],
                "default": null,
                "description": "The character encoding of the CSV data. Leave blank to default to <strong>UTF-8</strong>. See <a href=\"https://docs.python.org/3/library/codecs.html#standard-encodings\" target=\"_blank\">list of python encodings</a> for allowable options."
              },
              "double_quote": {
                "type": "boolean",
                "default": true,
                "description": "Whether two quotes in a quoted CSV value denote a single quote in the data."
              },
              "newlines_in_values": {
                "type": "boolean",
                "default": false,
                "description": "Whether newline characters are allowed in CSV values. Turning this on may affect performance. Leave blank to default to False."
              },
              "additional_reader_options": {
                "type": "string",
                "default": "{}",
                "description": "Optionally add a valid JSON string here to provide additional options to the csv reader. Mappings must correspond to options <a href=\"https://arrow.apache.org/docs/python/generated/pyarrow.csv.ConvertOptions.html#pyarrow.csv.ConvertOptions\" target=\"_blank\">detailed here</a>. 'column_types' is used internally to handle schema so overriding that would likely cause problems.",
                "examples": ["{\"timestamp_parsers\": [\"%m/%d/%Y %H:%M\", \"%Y/%m/%d %H:%M\"], \"strings_can_be_null\": true, \"null_values\": [\"NA\", \"NULL\"]}"]
              }
            }
          }
        ]
      }
    }
  }
}
